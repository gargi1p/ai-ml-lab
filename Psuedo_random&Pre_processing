{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1RkcazT44m-Linx2_AfTjUR7MvUDNDCZT","timestamp":1736189926867}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DGTeOeo-dqgC","executionInfo":{"status":"ok","timestamp":1736189915645,"user_tz":-330,"elapsed":10766,"user":{"displayName":"GARGI SINGH","userId":"05167182914036372664"}},"outputId":"e142d5ef-202f-4374-e02c-cbb3eab9f1ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Randomly Generated Multivariate Data (First 5 rows):\n","    Feature1  Feature2  Feature3  Feature4  Feature5\n","0  0.051682  0.531355  0.540635  0.637430  0.726091\n","1  0.975852  0.516300  0.322956  0.795186  0.270832\n","2  0.438971  0.078456  0.025351  0.962648  0.835980\n","3  0.695974  0.408953  0.173294  0.156437  0.250243\n","4  0.549227  0.714596  0.660197  0.279934  0.954865\n","Correlation coefficient between Feature3 and Feature4 (Random Data): -0.06174395615510341\n","Detected Outliers in Random Data:\n"," Empty DataFrame\n","Columns: [Feature1, Feature2, Feature3, Feature4, Feature5, Feature2_noisy, Feature2_smoothed]\n","Index: []\n","Reduced Random Data (First 5 rows):\n"," [[-0.06289285  0.3419371 ]\n"," [ 0.29289407 -0.28331398]\n"," [ 0.56646309 -0.12375156]\n"," [-0.1604065  -0.39453014]\n"," [ 0.07796464  0.4560301 ]]\n","Final Random DataFrame:\n","    Feature1  Feature2  Feature3  Feature4  Feature5  Feature2_noisy  \\\n","0  0.051682  0.531355  0.540635  0.637430  0.726091        0.452307   \n","1  0.975852  0.516300  0.322956  0.795186  0.270832        0.563447   \n","2  0.438971  0.078456  0.025351  0.962648  0.835980        0.266659   \n","3  0.695974  0.408953  0.173294  0.156437  0.250243        0.543495   \n","4  0.549227  0.714596  0.660197  0.279934  0.954865        0.873915   \n","\n","   Feature2_smoothed  Feature4_binned  \n","0           0.452307              1.0  \n","1           0.507877              2.0  \n","2           0.427471              2.0  \n","3           0.457867              0.0  \n","4           0.561356              0.0  \n","Correlation Matrix for Random Data:\n","                    Feature1  Feature2  Feature3  Feature4  Feature5  \\\n","Feature1           1.000000 -0.177701  0.060401  0.163218  0.081940   \n","Feature2          -0.177701  1.000000  0.163534 -0.119321  0.024978   \n","Feature3           0.060401  0.163534  1.000000 -0.061744  0.030609   \n","Feature4           0.163218 -0.119321 -0.061744  1.000000  0.256181   \n","Feature5           0.081940  0.024978  0.030609  0.256181  1.000000   \n","Feature2_noisy    -0.160356  0.936910  0.124284 -0.113171  0.026039   \n","Feature2_smoothed -0.087272  0.450802 -0.114898  0.056627 -0.159745   \n","Feature4_binned    0.215745 -0.144429 -0.114010  0.942470  0.238504   \n","\n","                   Feature2_noisy  Feature2_smoothed  Feature4_binned  \n","Feature1                -0.160356          -0.087272         0.215745  \n","Feature2                 0.936910           0.450802        -0.144429  \n","Feature3                 0.124284          -0.114898        -0.114010  \n","Feature4                -0.113171           0.056627         0.942470  \n","Feature5                 0.026039          -0.159745         0.238504  \n","Feature2_noisy           1.000000           0.508579        -0.158332  \n","Feature2_smoothed        0.508579           1.000000         0.069625  \n","Feature4_binned         -0.158332           0.069625         1.000000  \n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, KBinsDiscretizer\n","from sklearn.decomposition import PCA\n","from scipy.stats import pearsonr\n","\n","\n","\n","# Step 1: Random Number Generation\n","# Generating univariate, bivariate, and multivariate datasets\n","np.random.seed(42)\n","univariate_data = np.random.rand(100)\n","bivariate_data = np.random.rand(100, 2)\n","multivariate_data = np.random.rand(100, 5)\n","# Convert multivariate data to DataFrame\n","random_data = pd.DataFrame(multivariate_data, columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'])\n","\n","print(\"Randomly Generated Multivariate Data (First 5 rows):\\n\", random_data.head())\n","\n","\n","# Imputing missing values\n","imputer = SimpleImputer(strategy='mean')\n","random_data['Feature1'] = imputer.fit_transform(random_data[['Feature1']])\n","# 2.2 Noisy Data Handling\n","# Adding noise to 'Feature2'\n","random_data['Feature2_noisy'] = random_data['Feature2'] + np.random.normal(0, 0.1, size=random_data.shape[0])\n","# Smoothing noisy data using a rolling mean\n","random_data['Feature2_smoothed'] = random_data['Feature2_noisy'].rolling(window=3, min_periods=1).mean()\n","# Calculate correlation coefficient between two features\n","corr_coeff, _ = pearsonr(random_data['Feature3'], random_data['Feature4'])\n","print(f\"Correlation coefficient between Feature3 and Feature4 (Random Data): {corr_coeff}\")\n","\n","\n","\n","\n","# 2.4 Data Value Conflict Detection\n","# Detecting outliers using Z-scores for 'Feature3'\n","z_scores = np.abs((random_data['Feature3'] - random_data['Feature3'].mean()) / random_data['Feature3'].std())\n","random_outliers = random_data[z_scores > 3]\n","print(\"Detected Outliers in Random Data:\\n\", random_outliers)\n","\n","\n","\n","# 2.5 Data Reduction\n","# Using PCA for dimensionality reduction\n","pca = PCA(n_components=2)\n","random_data_reduced = pca.fit_transform(random_data[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']])\n","print(\"Reduced Random Data (First 5 rows):\\n\", random_data_reduced[:5])\n","\n","\n","# 2.6 Data Transformation\n","scaler = StandardScaler()\n","random_data_scaled = scaler.fit_transform(random_data[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']])\n","\n","# 2.7 Data Discretization\n","# Discretizing 'Feature4' into 3 bins\n","kbins = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n","random_data['Feature4_binned'] = kbins.fit_transform(random_data[['Feature4']])\n","# Output final Random DataFrame\n","print(\"Final Random DataFrame:\\n\", random_data.head())\n","\n","\n","\n","# 2.6 Data Transformation\n","scaler = StandardScaler()\n","random_data_scaled = scaler.fit_transform(random_data[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5']])\n","# 2.3 Data Integration: Redundancy and Correlation Analysis\n","# Calculate correlation matrix\n","random_correlation_matrix = random_data.corr()\n","print(\"Correlation Matrix for Random Data:\\n\", random_correlation_matrix)\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["\n","\n","# Step 2: Download and Read Kaggle Dataset\n","# Ensure you have the Kaggle dataset downloaded and accessible\n","# Example dataset: Titanic dataset\n","# Replace 'path/to/titanic.csv' with the actual file path\n","data = pd.read_csv('/content/titanic.csv')\n","print(data.columns)  # Print column names for reference\n","# Step 3: Data Pre-processing\n","# 3.1 Data Cleaning: Overview and Missing Values\n","# Handling missing values for 'Age' as an example\n","age_column_name = 'Age'\n","data.columns = data.columns.str.lower()\n","age_column_name = 'age'\n","imputer = SimpleImputer(strategy='mean')\n","data[age_column_name] = imputer.fit_transform(data[[age_column_name]])\n","\n","# 3.2 Noisy Data Handling\n","# Adding noise to 'fare' - Using lowercase 'fare' to match the column name\n","data['fare_noisy'] = data['fare'] + np.random.normal(0, 0.1, size=data.shape[0])\n","# Smoothing noisy data using a rolling mean - Using lowercase 'fare_noisy'\n","data['fare_smoothed'] = data['fare_noisy'].rolling(window=3, min_periods=1).mean()\n","# 3.2 Noisy Data Handling\n","# Adding noise to 'Fare' - Using lowercase 'fare' to match the column name\n","data['fare_noisy'] = data['fare'] + np.random.normal(0, 0.1, size=data.shape[0])\n","# Smoothing noisy data using a rolling mean - Using lowercase 'fare_noisy'\n","data['fare_smoothed'] = data['fare_noisy'].rolling(window=3, min_periods=1).mean()\n","# 4.3 Data Integration: Redundancy and Correlation Analysis\n","# Filter to include only numeric columns for correlation calculation\n","numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns\n","correlation_matrix = data[numeric_columns].corr()\n","print(\"Correlation Matrix:\\n\", correlation_matrix)\n","\n","# Calculate correlation coefficient between two features\n","if 'sibsp' in data.columns and 'Parch' in data.columns:\n","    corr_coeff, _ = pearsonr(data['sibsp'], data['parch'])\n","    print(f\"Correlation coefficient between sibsp and parch: {corr_coeff}\")\n","\n","#3.4 Data Value Conflict Detection\n","# Example: Detecting outliers using Z-scores in 'fare'\n","# Use 'fare' instead of 'Fare' to match the lowercase column name\n","z_scores = np.abs((data['fare'] - data['fare'].mean()) / data['fare'].std()) # Changed 'Fare' to 'fare'\n","outliers = data[z_scores > 3]\n","print(\"Detected Outliers:\\n\", outliers)\n","\n","# 3.5 Data Reduction\n","# Using PCA for dimensionality reduction (assuming numeric columns)\n","numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns\n","pca = PCA(n_components=2)\n","data_reduced = pca.fit_transform(data[numeric_columns])\n","print(\"Reduced Data (First 5 rows):\\n\", data_reduced[:5])\n","\n","# 3.6 Data Transformation\n","scaler = StandardScaler()\n","data_scaled = scaler.fit_transform(data[numeric_columns])\n","# 3.7 Data Discretization\n","# Discretizing 'Age' into 3 bins as an example\n","kbins = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n","# Use 'age' (lowercase) to match the column name after converting to lowercase\n","data['Age_binned'] = kbins.fit_transform(data[['age']]) # Changed 'Age' to 'age'\n","# Output final DataFrame\n","print(\"Final DataFrame:\\n\", data.head())\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kaL9OSO2phvJ","executionInfo":{"status":"ok","timestamp":1736186278009,"user_tz":-330,"elapsed":361,"user":{"displayName":"Nis S. Singh","userId":"03546528698528142979"}},"outputId":"d5073bf2-9cf0-4cd5-9c1a-d05e70597b29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n","       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n","       'alive', 'alone'],\n","      dtype='object')\n","Correlation Matrix:\n","                survived    pclass       age     sibsp     parch      fare  \\\n","survived       1.000000 -0.338481 -0.069809 -0.035322  0.081629  0.257307   \n","pclass        -0.338481  1.000000 -0.331339  0.083081  0.018443 -0.549500   \n","age           -0.069809 -0.331339  1.000000 -0.232625 -0.179191  0.091566   \n","sibsp         -0.035322  0.083081 -0.232625  1.000000  0.414838  0.159651   \n","parch          0.081629  0.018443 -0.179191  0.414838  1.000000  0.216225   \n","fare           0.257307 -0.549500  0.091566  0.159651  0.216225  1.000000   \n","fare_noisy     0.257299 -0.549330  0.091436  0.159753  0.216442  0.999998   \n","fare_smoothed  0.155316 -0.293135  0.044098  0.053799  0.110791  0.576355   \n","\n","               fare_noisy  fare_smoothed  \n","survived         0.257299       0.155316  \n","pclass          -0.549330      -0.293135  \n","age              0.091436       0.044098  \n","sibsp            0.159753       0.053799  \n","parch            0.216442       0.110791  \n","fare             0.999998       0.576355  \n","fare_noisy       1.000000       0.576324  \n","fare_smoothed    0.576324       1.000000  \n","Detected Outliers:\n","      survived  pclass     sex        age  sibsp  parch      fare embarked  \\\n","27          0       1    male  19.000000      3      2  263.0000        S   \n","88          1       1  female  23.000000      3      2  263.0000        S   \n","118         0       1    male  24.000000      0      1  247.5208        C   \n","258         1       1  female  35.000000      0      0  512.3292        C   \n","299         1       1  female  50.000000      0      1  247.5208        C   \n","311         1       1  female  18.000000      2      2  262.3750        C   \n","341         1       1  female  24.000000      3      2  263.0000        S   \n","377         0       1    male  27.000000      0      2  211.5000        C   \n","380         1       1  female  42.000000      0      0  227.5250        C   \n","438         0       1    male  64.000000      1      4  263.0000        S   \n","527         0       1    male  29.699118      0      0  221.7792        S   \n","557         0       1    male  29.699118      0      0  227.5250        C   \n","679         1       1    male  36.000000      0      1  512.3292        C   \n","689         1       1  female  15.000000      0      1  211.3375        S   \n","700         1       1  female  18.000000      1      0  227.5250        C   \n","716         1       1  female  38.000000      0      0  227.5250        C   \n","730         1       1  female  29.000000      0      0  211.3375        S   \n","737         1       1    male  35.000000      0      0  512.3292        C   \n","742         1       1  female  21.000000      2      2  262.3750        C   \n","779         1       1  female  43.000000      0      1  211.3375        S   \n","\n","     class    who  adult_male deck  embark_town alive  alone  fare_noisy  \\\n","27   First    man        True    C  Southampton    no  False  263.160051   \n","88   First  woman       False    C  Southampton   yes  False  263.220121   \n","118  First    man        True    B    Cherbourg    no  False  247.549959   \n","258  First  woman       False  NaN    Cherbourg   yes   True  512.038386   \n","299  First  woman       False    B    Cherbourg   yes  False  247.424062   \n","311  First  woman       False    B    Cherbourg   yes  False  262.432362   \n","341  First  woman       False    C  Southampton   yes  False  262.992993   \n","377  First    man        True    C    Cherbourg    no  False  211.686252   \n","380  First  woman       False  NaN    Cherbourg   yes   True  227.510499   \n","438  First    man        True    C  Southampton    no  False  263.069613   \n","527  First    man        True    C  Southampton    no   True  221.867066   \n","557  First    man        True  NaN    Cherbourg    no   True  227.566061   \n","679  First    man        True    B    Cherbourg   yes  False  512.384034   \n","689  First  child       False    B  Southampton   yes  False  211.444463   \n","700  First  woman       False    C    Cherbourg   yes  False  227.501018   \n","716  First  woman       False    C    Cherbourg   yes   True  227.511584   \n","730  First  woman       False    B  Southampton   yes   True  211.271125   \n","737  First    man        True    B    Cherbourg   yes   True  512.449143   \n","742  First  woman       False    B    Cherbourg   yes  False  262.392373   \n","779  First  woman       False    B  Southampton   yes  False  211.361243   \n","\n","     fare_smoothed  \n","27      100.559303  \n","88      101.816353  \n","118      92.103758  \n","258     225.918901  \n","299     143.174563  \n","311     134.112242  \n","341     108.121425  \n","377     100.384589  \n","380      79.783194  \n","438     105.394582  \n","527      80.092169  \n","557      97.836462  \n","679     189.707519  \n","689      76.517012  \n","700     115.322375  \n","716      82.695262  \n","730      81.807895  \n","737     187.655210  \n","742     123.727207  \n","779      77.223130  \n","Reduced Data (First 5 rows):\n"," [[-40.68206937 -15.2293913 ]\n"," [ 55.25937156  -7.33097087]\n"," [-34.30982776   5.31958456]\n"," [ 31.67015618   3.8689608 ]\n"," [-35.22776798  -0.39855827]]\n","Final DataFrame:\n","    survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n","0         0       3    male  22.0      1      0   7.2500        S  Third   \n","1         1       1  female  38.0      1      0  71.2833        C  First   \n","2         1       3  female  26.0      0      0   7.9250        S  Third   \n","3         1       1  female  35.0      1      0  53.1000        S  First   \n","4         0       3    male  35.0      0      0   8.0500        S  Third   \n","\n","     who  adult_male deck  embark_town alive  alone  fare_noisy  \\\n","0    man        True  NaN  Southampton    no  False    7.134965   \n","1  woman       False    C    Cherbourg   yes  False   71.131357   \n","2  woman       False  NaN  Southampton   yes   True    7.737867   \n","3  woman       False    C  Southampton   yes  False   53.114544   \n","4    man        True  NaN  Southampton    no   True    8.143494   \n","\n","   fare_smoothed  Age_binned  \n","0       7.134965         0.0  \n","1      39.133161         1.0  \n","2      28.668063         0.0  \n","3      43.994589         1.0  \n","4      22.998635         1.0  \n"]}]}]}